{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMgrgumeRHSb8ZWLIywqtp4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adithyaac/Captcha-breaker/blob/main/Task_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "IMAGE_SIZE = (150, 50)\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 50\n",
        "LEARNING_RATE = 0.001\n",
        "MAX_LENGTH = 10\n",
        "HIDDEN_SIZE = 256"
      ],
      "metadata": {
        "id": "xbpvaRaqhMOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(easy_dir, hard_dir):\n",
        "\n",
        "    image_paths = []\n",
        "    words = []\n",
        "\n",
        "    for directory in [easy_dir, hard_dir]:\n",
        "        for filename in os.listdir(directory):\n",
        "            if filename.endswith('.png'):\n",
        "\n",
        "                image_paths.append(os.path.join(directory, filename))\n",
        "\n",
        "                word = filename.split('_')[1].split('.')[0]\n",
        "                words.append(word)\n",
        "\n",
        "    return image_paths, words"
      ],
      "metadata": {
        "id": "hiVhuE7JhzP9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/adithyaac/Captcha-breaker.git\n",
        "\n",
        "DATASET_PATH = './Captcha-breaker/captcha_dataset'\n",
        "EASY_DIR = f'{DATASET_PATH}/easy'\n",
        "HARD_DIR = f'{DATASET_PATH}/hard'\n",
        "\n"
      ],
      "metadata": {
        "id": "BcHDVGOjij29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CAPTCHADataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_paths, words, transform=None):\n",
        "\n",
        "        self.image_paths = image_paths\n",
        "        self.words = words\n",
        "        self.transform = transform\n",
        "\n",
        "        # Create character dictionaries including both cases\n",
        "        all_chars = set(''.join(words))  # This will now include both upper and lower case\n",
        "        self.char_to_idx = {char: idx + 1 for idx, char in enumerate(sorted(all_chars))}\n",
        "        self.char_to_idx['<PAD>'] = 0  # Add padding token\n",
        "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
        "\n",
        "        print(f\"Number of unique characters (including case): {len(all_chars)}\")\n",
        "        print(\"Character set:\", sorted(all_chars))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load image\n",
        "        image = Image.open(self.image_paths[idx]).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert word to indices, preserving case\n",
        "        word = self.words[idx]\n",
        "        word_indices = [self.char_to_idx[c] for c in word]\n",
        "        # Pad sequence\n",
        "        word_indices = word_indices + [0] * (MAX_LENGTH - len(word_indices))\n",
        "\n",
        "        return image, torch.tensor(word_indices), len(word)"
      ],
      "metadata": {
        "id": "bFCH-QCdi8v4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CAPTCHACnnRnn(nn.Module):\n",
        "    def __init__(self, num_chars, hidden_size=HIDDEN_SIZE):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        # CNN Feature Extractor\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.BatchNorm2d(32),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.BatchNorm2d(64),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "\n",
        "        # Calculate CNN output size\n",
        "        self.cnn_output_size = 128 * (IMAGE_SIZE[0] // 8) * (IMAGE_SIZE[1] // 8)\n",
        "\n",
        "        # Linear layer to reduce CNN output size\n",
        "        self.reduce_dim = nn.Linear(self.cnn_output_size, hidden_size)\n",
        "\n",
        "        # LSTM layer\n",
        "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.out = nn.Linear(hidden_size, num_chars)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # CNN feature extraction\n",
        "        batch_size = x.size(0)\n",
        "        x = self.conv_layers(x)\n",
        "        x = x.view(batch_size, -1)  # Flatten\n",
        "        x = self.reduce_dim(x)\n",
        "\n",
        "        # Expand for sequence length\n",
        "        x = x.unsqueeze(1).repeat(1, MAX_LENGTH, 1)\n",
        "\n",
        "        # LSTM\n",
        "        lstm_out, _ = self.lstm(x)\n",
        "\n",
        "        # Output layer\n",
        "        output = self.out(lstm_out)\n",
        "        return output"
      ],
      "metadata": {
        "id": "bcMncLe8nRgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device):\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for images, labels, lengths in train_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "\n",
        "            # Reshape outputs and labels for loss calculation\n",
        "            batch_size, seq_len, num_chars = outputs.size()\n",
        "            loss = criterion(outputs.view(-1, num_chars), labels.view(-1))\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(epoch_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct_chars = 0\n",
        "        total_chars = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels, lengths in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs.view(-1, outputs.size(-1)),\n",
        "                               labels.view(-1))\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Calculate character accuracy\n",
        "                _, predicted = outputs.max(2)\n",
        "                for pred, label, length in zip(predicted, labels, lengths):\n",
        "                    correct_chars += (pred[:length] == label[:length]).sum().item()\n",
        "                    total_chars += length\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "        accuracy = correct_chars / total_chars * 100\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
        "        print(f'Train Loss: {epoch_loss:.4f}')\n",
        "        print(f'Val Loss: {val_loss:.4f}')\n",
        "        print(f'Character Accuracy: {accuracy:.2f}%')\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save(model.state_dict(), 'best_model.pth', weights_only=True)\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "7qJl3At7nqhw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "NvwdXnqJntmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_paths, words = load_dataset(EASY_DIR, HARD_DIR)\n",
        "\n",
        "train_paths, test_paths, train_words, test_words = train_test_split(\n",
        "    image_paths, words, test_size=0.2, random_state=42\n",
        ")\n",
        "train_paths, val_paths, train_words, val_words = train_test_split(\n",
        "    train_paths, train_words, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "JXNMUMk6nzsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(IMAGE_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                       std=[0.229, 0.224, 0.225])\n",
        "])\n"
      ],
      "metadata": {
        "id": "NVuEU2uAn9uj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = CAPTCHADataset(train_paths, train_words, transform)\n",
        "val_dataset = CAPTCHADataset(val_paths, val_words, transform)\n",
        "test_dataset = CAPTCHADataset(test_paths, test_words, transform)\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "HY3YJn-VoGNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "num_chars = len(train_dataset.char_to_idx)\n",
        "model = CAPTCHACnnRnn(num_chars).to(device)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # ignore padding\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "\n",
        "# Train model\n",
        "history = train_model(model, train_loader, val_loader, criterion,\n",
        "                     optimizer, NUM_EPOCHS, device)"
      ],
      "metadata": {
        "id": "0iEd2-XyoJAU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}